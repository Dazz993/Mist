op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,1573.586,2201.134,0.008,20.000,270.000,20.018,19.982,330.000
enc-1st-layernorm,124.109,345.808,20.000,40.000,0.000,20.016,0.000,160.000
enc-attention-qkv,1886.475,3189.236,40.000,50.000,75.000,30.000,20.000,110.000
enc-attention-score,507.605,733.405,50.000,190.000,0.000,160.000,160.000,350.000
enc-attention-softmax,391.829,518.638,190.000,190.000,0.000,160.000,0.000,480.000
enc-attention-dropout,616.252,521.606,190.000,190.000,0.000,240.000,0.000,640.000
enc-attention-context,449.401,795.788,190.000,30.000,0.000,10.000,10.000,190.000
enc-attention-dense,1850.414,496.709,30.000,40.010,25.000,20.000,0.000,0.000
enc-post-attention-dropout,233.537,124.007,40.010,20.000,0.000,30.000,10.000,160.000
enc-2nd-layernorm,117.826,333.923,20.000,40.000,0.000,20.016,0.000,160.000
enc-MLP-GEMM-1,2413.851,3617.758,40.000,60.020,100.000,40.000,0.000,140.000
enc-MLP-gelu,118.309,232.321,60.020,60.000,0.000,40.000,0.000,240.000
enc-MLP-GEMM-2,3605.455,2084.529,60.000,40.010,100.000,20.000,0.000,100.000
enc-post-MLP-dropout,236.624,124.401,40.010,20.000,0.000,30.000,10.000,160.000
final-layernorm,122.941,287.551,20.000,20.000,0.000,20.016,0.000,80.000
gpt-post-process,8071.858,7168.257,20.000,0.000,250.000,200.025,99.975,0.000
