op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,1856.995,2243.316,0.016,48.000,99.000,48.035,143.965,336.000
enc-1st-layernorm,265.723,779.128,48.000,96.000,0.000,48.031,0.000,384.000
enc-attention-qkv,1466.393,2297.521,96.000,66.000,27.000,18.000,2.000,98.000
enc-attention-score,326.973,627.548,66.000,182.000,0.000,128.000,128.000,276.000
enc-attention-softmax,312.895,418.699,182.000,182.000,0.000,128.000,0.000,384.000
enc-attention-dropout,483.078,416.970,182.000,182.000,0.000,192.000,0.000,512.000
enc-attention-context,388.694,597.990,182.000,54.000,0.000,6.000,14.000,148.000
enc-attention-dense,1341.635,423.414,54.000,96.012,9.000,48.000,0.000,144.000
enc-post-attention-dropout,540.960,286.907,96.012,48.000,0.000,72.000,48.000,274.000
enc-2nd-layernorm,264.692,779.831,48.000,96.000,0.000,48.031,0.000,384.000
enc-MLP-GEMM-1,1696.706,2739.722,96.000,72.006,36.000,24.000,0.000,120.000
enc-MLP-gelu,74.637,153.822,72.006,72.000,0.000,24.000,0.000,178.000
enc-MLP-GEMM-2,2606.791,1493.037,72.000,96.012,36.000,48.000,0.000,168.000
enc-post-MLP-dropout,546.908,287.843,96.012,48.000,0.000,72.000,48.000,274.000
final-layernorm,436.366,591.564,48.000,48.000,0.000,96.031,0.000,192.000
gpt-post-process,4741.174,4598.153,48.000,0.000,75.000,100.051,49.949,0.000
