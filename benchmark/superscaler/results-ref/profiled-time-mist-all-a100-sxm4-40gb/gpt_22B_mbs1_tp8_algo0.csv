op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,1169.747,1632.816,0.016,48.000,123.000,48.035,0.000,308.000
enc-1st-layernorm,126.994,441.527,48.000,96.000,0.000,48.031,0.000,384.000
enc-attention-qkv,531.751,1139.265,96.000,66.000,27.000,18.000,2.000,98.000
enc-attention-score,375.360,494.480,66.000,310.000,0.000,256.000,256.000,532.000
enc-attention-softmax,619.203,623.417,310.000,310.000,0.000,256.000,0.000,768.000
enc-attention-dropout,577.831,491.285,310.000,310.000,0.000,384.000,0.000,1024.000
enc-attention-context,258.034,612.164,310.000,54.000,0.000,6.000,14.000,276.000
enc-attention-dense,716.007,170.314,54.000,96.012,9.000,48.000,0.000,144.000
enc-post-attention-dropout,336.444,172.663,96.012,48.000,0.000,72.000,48.000,300.000
enc-2nd-layernorm,126.511,442.350,48.000,96.000,0.000,48.031,0.000,384.000
enc-MLP-GEMM-1,645.036,1301.271,96.000,72.006,36.000,24.000,0.000,120.000
enc-MLP-gelu,49.692,103.283,72.006,72.000,0.000,24.000,0.000,192.000
enc-MLP-GEMM-2,1238.120,607.479,72.000,96.012,36.000,48.000,0.000,168.000
enc-post-MLP-dropout,336.307,172.830,96.012,48.000,0.000,72.000,48.000,300.000
final-layernorm,126.958,328.755,48.000,48.000,0.000,48.031,0.000,192.000
gpt-post-process,2347.481,2380.234,48.000,0.000,75.000,100.051,49.949,0.000
