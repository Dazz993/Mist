op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,7113.099,2302.217,0.008,20.000,145.000,20.018,0.000,226.000
enc-1st-layernorm,137.389,449.824,20.000,40.000,0.000,20.016,0.000,160.000
enc-attention-qkv,1281.172,9227.890,40.000,35.000,37.500,15.000,21.000,158.000
enc-attention-score,245.684,463.039,35.000,105.000,0.000,80.000,80.000,180.000
enc-attention-softmax,3402.007,5507.350,105.000,105.000,0.000,240.000,160.000,560.000
enc-attention-dropout,443.035,434.130,105.000,105.000,0.000,120.000,0.000,320.000
enc-attention-context,255.531,440.001,105.000,25.000,0.000,5.000,0.000,90.000
enc-attention-dense,7192.826,893.068,25.000,40.010,12.500,20.000,0.000,94.000
enc-post-attention-dropout,336.355,201.267,40.010,20.000,0.000,30.000,20.000,140.000
enc-2nd-layernorm,137.365,449.693,20.000,40.000,0.000,20.016,0.000,160.000
enc-MLP-GEMM-1,1584.321,9904.146,40.000,40.010,50.000,20.000,0.000,130.000
enc-MLP-gelu,95.743,185.007,40.010,40.000,0.000,20.000,0.000,140.000
enc-MLP-GEMM-2,8336.377,3134.298,40.000,40.010,50.000,20.000,0.000,130.000
enc-post-MLP-dropout,336.307,200.152,40.010,20.000,0.000,30.000,20.000,140.000
final-layernorm,137.734,319.916,20.000,20.000,0.000,20.016,0.000,80.000
gpt-post-process,6499.636,15094.227,20.000,0.000,125.000,100.025,49.975,126.000
