diff --git a/megatron/arguments.py b/megatron/arguments.py
index 0ca8776e..85c2ae5f 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -588,6 +588,8 @@ def _add_network_size_args(parser):
                        'This is set to 4*hidden-size if not provided')
     group.add_argument('--num-attention-heads', type=int, default=None,
                        help='Number of transformer attention heads.')
+    group.add_argument('--parallel-block', action='store_true',
+                        help='Use parallel block for transformer layers.')
     group.add_argument('--kv-channels', type=int, default=None,
                        help='Projection weights dimension in multi-head '
                        'attention. This is set to '
diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index adccd440..0cac5b40 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -144,6 +144,7 @@ class TransformerConfig(ModelParallelConfig):
     kv_channels: int = None
     hidden_dropout: float = 0.1
     attention_dropout: float = 0.1
+    parallel_block: bool = False
     fp32_residual_connection: bool = False
     # @jcasper should we keep this option?
     apply_residual_connection_post_layernorm: bool = False
diff --git a/megatron/model/transformer.py b/megatron/model/transformer.py
index 9f1144c0..acedfbbb 100644
--- a/megatron/model/transformer.py
+++ b/megatron/model/transformer.py
@@ -6,6 +6,7 @@ import math
 import numpy as np
 import torch
 import torch.nn.functional as F
+import xformers.ops as xops
 from typing import Optional
 
 from megatron import get_timers, get_args, get_retro_args, core, get_num_microbatches
@@ -427,6 +428,95 @@ class CoreAttention(MegatronModule):
 
         return context_layer
 
+# Function Interface for tracing
+def xformer_memory_efficient_attention(
+    qkv, causal=True, key_padding_mask=None, dropout=0.0, softmax_scale=None
+):
+    """Implements the multihead softmax attention.
+    Arguments
+    ---------
+        qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)
+        causal: if passed, will override self.causal
+        key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
+            False means to mask out. (B, S)
+    """
+    q, k, v = qkv.unbind(dim=2)
+    batch_size, seqlen, num_heads, head_dim = q.shape
+    softmax_scale = softmax_scale or 1.0 / math.sqrt(q.shape[-1])
+
+    # Compute the bias
+    attn_bias = None
+    if key_padding_mask is not None:
+        padding_mask = torch.full(
+            (batch_size, seqlen), -10000.0, dtype=qkv.dtype, device=qkv.device
+        )
+        padding_mask.masked_fill_(key_padding_mask, 0.0)
+        attn_bias = padding_mask.reshape(batch_size, 1, 1, seqlen)
+        # Expand the mask to the same shape of q
+        attn_bias = attn_bias.expand(-1, num_heads, seqlen, -1)
+
+    if causal and attn_bias is None:
+        attn_bias = xops.LowerTriangularMask()
+    elif causal:
+        # ===============================================================================
+        # Only works with FwOp
+        # attn_bias = xops.fmha.attn_bias.LowerTriangularMaskWithTensorBias(attn_bias)
+        # ===============================================================================
+        # Compatibility for both FwOp and BwOp
+        causal_mask = torch.triu(
+            torch.full((seqlen, seqlen), -10000.0, device=qkv.device, dtype=qkv.dtype),
+            1,
+        )
+        attn_bias = attn_bias + causal_mask
+
+    # Compute the attention
+    output = xops.memory_efficient_attention(
+        query=q,
+        key=k,
+        value=v,
+        attn_bias=attn_bias,
+        p=dropout,
+        scale=softmax_scale,
+        op=(xops.fmha.cutlass.FwOp, xops.fmha.cutlass.BwOp),
+    )
+
+    return output
+
+class XFormersSelfAttention(torch.nn.Module):
+    """Implement the scaled dot product attention with softmax.
+    Arguments
+    ---------
+        softmax_scale: The temperature to use for the softmax attention.
+                      (default: 1/sqrt(d_keys) where d_keys is computed at
+                      runtime)
+        attention_dropout: The dropout rate to apply to the attention
+                           (default: 0.0)
+    """
+    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0,
+                 device=None, dtype=None):
+        super().__init__()
+        assert rearrange is not None, 'Please install einops first, e.g., with pip install einops'
+        self.causal = causal
+        self.softmax_scale = softmax_scale
+        self.dropout_p = attention_dropout
+
+    def forward(self, q, k, v, attention_mask=None):
+        """Implements the multihead softmax attention.
+        Arguments
+        ---------
+            q, k, v: The tensor containing the query, key, and value. (B, S, H, D)
+        """
+
+        assert all((i.dtype in [torch.float16, torch.bfloat16] for i in (q,k,v)))
+        assert all((i.is_cuda for i in (q,k,v)))
+
+        # Stack into (B, S, 3, H, D)
+        qkv = torch.stack([q, k, v], dim=2)
+        output = xformer_memory_efficient_attention(
+            qkv, causal=self.causal, key_padding_mask=attention_mask,
+            dropout=self.dropout_p, softmax_scale=self.softmax_scale
+        )
+        return output
 
 class FlashSelfAttention(torch.nn.Module):
     """Implement the scaled dot product attention with softmax.
@@ -520,6 +610,14 @@ class ParallelAttention(MegatronModule):
         self.use_flash_attn = args.use_flash_attn \
             and attention_type == AttnType.self_attn \
             and self.attn_mask_type == AttnMaskType.causal
+        # Dispatch efficient attention kernels
+        device_name = torch.cuda.get_device_name().split(" ")[-1].lower()
+        if device_name.startswith(("v100",)):
+            self.use_xformer_attn = self.use_flash_attn
+            self.use_flash_attn = False
+        else:
+            self.use_xformer_attn = False
+            self.use_flash_attn = self.use_flash_attn
         if self.use_flash_attn:
             if flash_attn_unpadded_func is None:
                 raise ImportError('FlashAttention is not installed, please install with '
@@ -530,6 +628,11 @@ class ParallelAttention(MegatronModule):
                                                                 'supports causal mask for now')
             if rearrange is None:
                 raise ImportError('einops is not installed, please install with pip install einops')
+        if self.use_xformer_attn:
+            assert attention_type == AttnType.self_attn, ('XFormersAttention code path only supports '
+                                                          'self-attention for now')
+            assert self.attn_mask_type == AttnMaskType.causal, ('XFormersAttention code path only '
+                                                                'supports causal mask for now')
 
         # Per attention head and per partition values.
         world_size = mpu.get_tensor_model_parallel_world_size()
@@ -584,7 +687,11 @@ class ParallelAttention(MegatronModule):
         self.checkpoint_core_attention = config.recompute_granularity == 'selective'
 
         if self.use_flash_attn:
-            self.core_attention_flash = FlashSelfAttention(
+                self.core_attention_flash = FlashSelfAttention(
+                    causal=True, attention_dropout=config.attention_dropout
+                )
+        elif self.use_xformer_attn:
+            self.core_attention_xformer = XFormersSelfAttention(
                 causal=True, attention_dropout=config.attention_dropout
             )
 
@@ -788,13 +895,22 @@ class ParallelAttention(MegatronModule):
             # otherwise, only relative positional embedding takes effect
             # value_layer = apply_rotary_pos_emb(value_layer, k_pos_emb)
 
-        if not self.use_flash_attn:
+        if not self.use_flash_attn and not self.use_xformer_attn:
             if self.checkpoint_core_attention:
                 context_layer = self._checkpointed_attention_forward(
                     query_layer, key_layer, value_layer, attention_mask)
             else:
                 context_layer = self.core_attention(
                     query_layer, key_layer, value_layer, attention_mask)
+        elif self.use_xformer_attn:
+            q, k, v = [rearrange(x, 's b ... -> b s ...').contiguous()
+                       for x in (query_layer, key_layer, value_layer)]
+            if not self.sequence_parallel:
+                with tensor_parallel.get_cuda_rng_tracker().fork():
+                    context_layer = self.core_attention_xformer(q, k, v)
+            else:
+                context_layer = self.core_attention_xformer(q, k, v)
+            context_layer = rearrange(context_layer, 'b s h d -> s b (h d)').contiguous()
         else:
             q, k, v = [rearrange(x, 's b ... -> b s ...').contiguous()
                        for x in (query_layer, key_layer, value_layer)]
@@ -867,6 +983,7 @@ class ParallelTransformerLayer(MegatronModule):
 
         self.bf16 = config.bf16
         self.fp32_residual_connection = config.fp32_residual_connection
+        self.parallel_block = config.parallel_block
 
         # Normalize the input data.
         self.input_norm = get_norm(config)
@@ -882,7 +999,9 @@ class ParallelTransformerLayer(MegatronModule):
         self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else None
 
         # Normalize the attention output
-        self.post_attention_norm = get_norm(config)
+        # (if not using parallel block, then use the same norm of MLP).
+        if not self.parallel_block:
+            self.post_attention_norm = get_norm(config)
 
         # Cross attention.
         if self.layer_type in (LayerType.decoder,
@@ -1140,6 +1259,18 @@ class ParallelTransformerLayer(MegatronModule):
                 retriever_attn_mask=None,
                 inference_params=None,
                 rotary_pos_emb=None):
+        # jit scripting for a nn.module (with dropout) is not
+        # trigerring the fusion kernel. For now, we use two
+        # different nn.functional routines to account for varying
+        # dropout semantics during training and inference phases.
+        if self.bias_dropout_fusion:
+            if self.training:
+                bias_dropout_add_func = bias_dropout_add_fused_train
+            else:
+                bias_dropout_add_func = bias_dropout_add_fused_inference
+        else:
+            bias_dropout_add_func = get_bias_dropout_add(self.training)
+
         # hidden_states: [s, b, h]
 
         # Layer norm at the beginning of the transformer layer.
@@ -1159,35 +1290,26 @@ class ParallelTransformerLayer(MegatronModule):
         else:
             residual = hidden_states
 
-        if self.drop_path is None:
-            # jit scripting for a nn.module (with dropout) is not
-            # trigerring the fusion kernel. For now, we use two
-            # different nn.functional routines to account for varying
-            # dropout semantics during training and inference phases.
-            if self.bias_dropout_fusion:
-                if self.training:
-                    bias_dropout_add_func = bias_dropout_add_fused_train
-                else:
-                    bias_dropout_add_func = bias_dropout_add_fused_inference
-            else:
-                bias_dropout_add_func = get_bias_dropout_add(self.training)
-
-            if attention_bias is not None:
-                attention_bias = attention_bias.expand_as(residual)
-            with self.bias_dropout_add_exec_handler():
-                norm_input = bias_dropout_add_func(
-                    attention_output,
-                    attention_bias,
-                    residual,
-                    self.hidden_dropout)
+        if self.parallel_block:
+            norm_input = attention_output
         else:
-            out = torch.nn.functional.dropout(attention_output + attention_bias,
-                                              p=self.hidden_dropout,
-                                              training=self.training)
-            norm_input = residual + self.drop_path(out)
+            if self.drop_path is None:
+                if attention_bias is not None:
+                    attention_bias = attention_bias.expand_as(residual)
+                with self.bias_dropout_add_exec_handler():
+                    norm_input = bias_dropout_add_func(
+                        attention_output,
+                        attention_bias,
+                        residual,
+                        self.hidden_dropout)
+            else:
+                out = torch.nn.functional.dropout(attention_output + attention_bias,
+                                                p=self.hidden_dropout,
+                                                training=self.training)
+                norm_input = residual + self.drop_path(out)
 
-        # Layer norm post the self attention.
-        norm_output = self.post_attention_norm(norm_input)
+            # Layer norm post the self attention.
+            norm_output = self.post_attention_norm(norm_input)
 
         # Cross attention.
         if self.layer_type == LayerType.encoder:
@@ -1226,7 +1348,9 @@ class ParallelTransformerLayer(MegatronModule):
         mlp_output, mlp_bias = self.mlp(norm_output)
 
         # Second residual connection.
-        if self.apply_residual_connection_post_norm:
+        if self.parallel_block:
+            mlp_output = mlp_output + attention_output
+        elif self.apply_residual_connection_post_norm:
             residual = norm_output
         else:
             residual = norm_input
diff --git a/megatron/optimizer/optimizer.py b/megatron/optimizer/optimizer.py
index 23749959..96ede942 100644
--- a/megatron/optimizer/optimizer.py
+++ b/megatron/optimizer/optimizer.py
@@ -7,6 +7,8 @@ from abc import abstractmethod
 from apex.multi_tensor_apply import multi_tensor_applier
 import amp_C
 import torch
+import torch.distributed
+import warnings
 
 from megatron import get_timers
 from megatron import print_rank_0
@@ -14,7 +16,7 @@ from megatron.core import mpu, tensor_parallel
 from megatron.model import Float16Module
 from megatron.model.module import param_is_not_shared
 
-from .clip_grads import clip_grad_norm_fp32, count_zeros_fp32
+from .clip_grads import clip_grad_norm_fp32, count_zeros_fp32, param_is_not_tensor_parallel_duplicate
 
 
 def _zero_grad_group_helper(group, set_to_none):
@@ -48,6 +50,53 @@ def _multi_tensor_copy_this_to_that(this, that, overflow_buf=None):
         for this_, that_ in zip(this, that):
             that_.copy_(this_)
 
+@torch.no_grad()
+def chunked_count_zeros(tensor, chunk_size=8 * 1024 ** 2):
+    """Count zeros in a tensor in a chunked manner."""
+    tensor = tensor.flatten()
+    tensor_size = tensor.numel()
+    num_chunks = (tensor.numel() + chunk_size - 1) // chunk_size
+    num_zeros = torch.cuda.FloatTensor([0.0])
+    for i in range(num_chunks):
+        chunk = tensor[i * chunk_size: min((i + 1) * chunk_size, tensor_size)]
+        num_zeros += chunk.numel() - torch.count_nonzero(chunk)
+    return int(num_zeros.item())
+
+
+@torch.no_grad()
+def count_all_and_zeros_fp32(parameters, model_parallel_group):
+    if isinstance(parameters, torch.Tensor):
+        parameters = [parameters]
+
+    # Filter parameters based on:
+    #   - grad should not be none
+    #   - parameter should not be shared
+    #   - should not be a replica due to tensor model parallelism
+    total_num_zeros = torch.cuda.FloatTensor([0.0])
+    total_num_grads = torch.cuda.FloatTensor([0.0])
+    for param in parameters:
+        grad_not_none = param.grad is not None
+        is_not_shared = param_is_not_shared(param)
+        is_not_tp_duplicate = param_is_not_tensor_parallel_duplicate(param)
+        if grad_not_none and is_not_shared and is_not_tp_duplicate:
+            grad = param.grad.detach()
+            # num_zeros = grad.numel() - torch.count_nonzero(grad)
+            num_zeros = chunked_count_zeros(grad)
+            total_num_zeros = num_zeros + total_num_zeros
+            total_num_grads = grad.numel() + total_num_grads
+
+    # Sum across all model-parallel GPUs.
+    torch.distributed.all_reduce(total_num_zeros,
+                                 op=torch.distributed.ReduceOp.SUM,
+                                 group=model_parallel_group)
+    torch.distributed.all_reduce(total_num_grads,
+                                    op=torch.distributed.ReduceOp.SUM,
+                                    group=model_parallel_group)
+
+    total_num_zeros = int(total_num_zeros.item())
+    total_num_grads = int(total_num_grads.item())
+
+    return total_num_zeros, total_num_grads
 
 
 class MegatronOptimizer(ABC):
@@ -309,6 +358,10 @@ class MixedPrecisionOptimizer(MegatronOptimizer):
             # so we can update the loss scale.
             self.grad_scaler.update(found_inf_flag)
 
+            if found_inf_flag:
+                # raise OverflowError(f"Grads overflow in benchmarking, the GradScaler is set to be the constant one. Check the overflow issue.")
+                warnings.warn(f"Grads overflow in benchmarking, the GradScaler is set to be the constant one. Check the overflow issue.")
+
             # If we found inf/nan, skip the update.
             if found_inf_flag:
                 return False, None, None
@@ -329,6 +382,13 @@ class MixedPrecisionOptimizer(MegatronOptimizer):
                             self.log_num_zeros_in_grad else None
         timers('optimizer-count-zeros').stop()
 
+        # ======================
+        # Added by Zhanda
+        num_zeros_in_grad, num_elem_in_grad = count_all_and_zeros_fp32(self.get_parameters(), model_parallel_group=self.get_model_parallel_group())
+        if torch.distributed.get_rank(self.get_model_parallel_group()) == 0:
+            print(f"[RANK={torch.distributed.get_rank()}] {num_zeros_in_grad=}, {num_elem_in_grad=}. Ratio: {num_zeros_in_grad / num_elem_in_grad * 100:.2f}%")
+        # ======================
+
         # Step the optimizer.
         timers('optimizer-inner-step', log_level=1).start(
             barrier=args.barrier_with_L1_time)
