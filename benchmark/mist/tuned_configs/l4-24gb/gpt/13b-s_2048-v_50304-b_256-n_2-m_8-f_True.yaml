print_config: true
ignore_warnings: true
seed: null
name: null
training:
  max_sequence_length: 2048
  vocab_size: 50304
  global_batch_size: 256
  params_dtype: float16
  exec_dtype: float16
  optimizer_dtype: float32
  autocast_enabled: false
  optimizer_name: adamw
hardware:
  gpu_type: null
  num_nodes: 2
  num_gpus_per_node: 8
  gpu_gpu_comm_params:
  - 4.5
  - 0.0
  - 2.964
  - 0.0001
  - 4.1635
  - 0.0004
  - 4.6832
  - 0.0003
  - 0.4895
  - 0.0045
  - 0.9791
  - 0.0076
  - 1.7183
  - 0.0
  - 3.3215
  - 0.0013
  - 0.4609
  - 0.0102
  - 0.9195
  - 0.0008
  - 1.6759
  - 0.0045
  - 2.9659
  - 0.003
  - 4.5
  - 0.0
  - 4.5
  - 0.0
  - 4.5
  - 0.0
  - 4.5
  - 0.0
  cpu_gpu_comm_params:
  - 5.7546
  - 0.0004
  gpu_cpu_comm_params:
  - 6.1252
  - 0.0004
  interference_model_params:
  - 18.9356
  - 1.2639
  - 50.0
  - 1.6046
  - 2.0865
  - 1.0009
  - 2.393
  - 2.4146
  - 1.0869
  - 2.6637
  - 1.9381
  - 3.2653
  - 1.0
  - 1.234
  - 50.0
  - 1.6883
  - 1.1676
  - 1.0
  - 1.0152
  - 1.0005
  - 1.0179
  - 1.0022
  - 1.0
  - 2.1383
  - 1.078
  - 2.438
  - 2.4327
  - 1.0974
  nvlink: false
  memory_capacity: 22.488
strategy:
  enabled: true
  layer_partitions:
  - 10
  - 10
  - 10
  - 10
  device_assignment:
  - - 1
    - 4
  - - 1
    - 4
  - - 1
    - 4
  - - 1
    - 4
  gradient_checkpointing:
  - 2
  - 1
  - 0
  - 0
  gradient_accumulation_steps: 64
  stage_strategies:
  - - 1
    - 4
    - 1
    - 0
    - 0
    - 1
    - 0.0
    - 0.0
    - 1.0
    - 0.625
  - - 1
    - 4
    - 1
    - 0
    - 0
    - 1
    - 0.0
    - 0.0
    - 0.875
    - 0.5
  - - 1
    - 4
    - 1
    - 0
    - 0
    - 1
    - 0.0
    - 0.0
    - 1.0
    - 0.125
  - - 1
    - 4
    - 1
    - 0
    - 0
    - 1
    - 0.0
    - 0.0
    - 0.875
    - 0.0
  pre_post_strategy: preset
  pre_post_strategy_array:
  - 1
  - 4
  - 1
  - 0
  - 0
  - 1
  - 0
  - 0
  - 0
  - 0
tuning:
  enabled: true
  zero_2_and_3_enabled: true
  activation_checkpointing_tuning_enabled: true
  state_offloading_enabled: true
  activation_offloading_enabled: true
  tuning_granularity: uniform-device-pp-mip
  sample_size: 30
  pre_post_strategy: dp
  pre_post_strategy_array: null
overlap: true
use_memory_buffer: true
nccl_timeout: 120
profile: false
tiny_bench: true
memory_debug: false
enable_advanced_opt_in_first_block: true
disable_tp_tuning: true
model:
  fp32_residual_connection: false
  fp16_lm_cross_entropy: true
  vocab_size: 50304
  num_tokentypes: 0
  position_embedding_type: absolute
  padding_idx: null
  rotary_emb_fraction: 0.0
  rotary_emb_base: 10000.0
  rotary_emb_scale_base: None
  rotary_emb_interleaved: false
  tie_word_embeddings: false
  add_encoder: true
  add_decoder: false
  encoder_attn_mask_type: causal
  add_pooler: false
  num_experts: null
  parallel_block: false
  parallel_block_tied_norm: false
  prenorm: true
  normalization: layernorm
  activation_function: gelu_fast
  mlp_type: mlp
  qkv_proj_bias: false
  out_proj_bias: false
  mlp_fc1_bias: false
  mlp_fc2_bias: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  scale_attn_weights: true
  use_flash_attn: true
  bias_dropout_fusion: false
  name: gpt2-13b
  hidden_size: 5120
  intermediate_size: 20480
  num_hidden_layers: 40
  num_attention_heads: 40