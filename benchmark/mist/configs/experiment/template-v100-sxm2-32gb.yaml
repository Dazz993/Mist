# @package _global_

training:
  max_sequence_length: 2048
  vocab_size: 50304
  global_batch_size: 64
  params_dtype: float16
  exec_dtype: float16
  optimizer_dtype: float32
  autocast_enabled: false
  optimizer_name: adamw

hardware:
  gpu_type: null
  num_nodes: 1
  num_gpus_per_node: 8
  gpu_gpu_comm_params: [4.5, 0.0, 15.6803, 0.0007, 40.2922, 0.0, 50.6314, 0.0, 0.9767, 0.0007, 2.5278, 0.0046, 4.9768, 0.003, 9.694, 0.0025, 1.1313, 0.0063, 2.2718, 0.0027, 4.5509, 0.0033, 9.737, 0.0049, 4.5, 0.0, 4.5, 0.0, 4.5, 0.0, 4.5, 0.0]
  cpu_gpu_comm_params: [5.7488, 0.0003]
  gpu_cpu_comm_params: [5.8382, 0.0004]
  interference_model_params: [1.3036, 1.0041, 2.1253, 2.0893, 1.2606, 1.0, 1.4759, 1.3305, 1.012, 1.5031, 1.0064, 1.3207, 1.2788, 1.0, 1.8277, 1.7078, 1.1698, 1.004, 1.0018, 1.0015, 1.0041, 1.0037, 1.0, 1.2916, 1.0, 1.2694, 1.2495, 1.2598]
  nvlink: false
  memory_capacity: 32.00

strategy:
  enabled: true
  layer_partitions: ["${model.num_hidden_layers}"]
  device_assignment: [[1, 8]]
  gradient_checkpointing: [0]
  gradient_accumulation_steps: 2
  stage_strategies:
  - [4, 8, 1, 0, 0, 1, 0, 0, 0, 0.0]
  pre_post_strategy: preset
  pre_post_strategy_array: [4, 8, 1, 0, 0, 1, 0, 0, 0, 0]


tuning:
  enabled: true
  zero_2_and_3_enabled: false
  activation_checkpointing_tuning_enabled: false
  state_offloading_enabled: true
  activation_offloading_enabled: true
  tuning_granularity: uniform-device-pp-mip   # no-pp, uniform-pp, uniform-device-pp, inter-stage
  sample_size: 30
  pre_post_strategy: dp   # ["dp", "intra-node-tp-with-ore", "intra-node-tp-without-ore"]
  pre_post_strategy_array: null

overlap: true
use_memory_buffer: true
nccl_timeout: 120

profile: false
tiny_bench: true
memory_debug: false

enable_advanced_opt_in_first_block: true
disable_tp_tuning: false